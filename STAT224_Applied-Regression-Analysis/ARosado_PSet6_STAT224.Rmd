---
title: 'STAT 224: PSet 6'
author: "Avery I. Rosado"
date: "5/17/2022"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE, echo=FALSE}
rm(list=ls())
library(stat224)
library(dplyr)
library(plotrix)
library(data.table)
library(car)
library(MASS)
library(gridExtra)
```

```{r}
tinytex::parse_packages(
  text = "! LaTeX Error: File `ocgbase.sty' not found."
)
```

## Problem 1

### a.

```{r}
corndata <- P148
# F1 <- data.frame(corndata %>% filter(!(Fertilizer %in% c(2,3,4))))
# F2 <- corndata %>% filter(!(Fertilizer %in% c(1,3,4)))
# F3 <- corndata %>% filter(!(Fertilizer %in% c(1,2,4)))
# control <- corndata %>% filter(!(Fertilizer %in% c(1,2,3)))

corndata$F1 <- ifelse(corndata$Fertilizer == 1,1,0)
corndata$F2 <- ifelse(corndata$Fertilizer == 2,1,0)
corndata$F3 <- ifelse(corndata$Fertilizer == 3,1,0)
corndata$control <- ifelse(corndata$Fertilizer == 4,1,0)
```

### b.

Each indicator represents the corn yield data for specific groups of fertilizer. The textbook authors likely chose these symbols because each coefficient is equivalent to the sample mean for each fertilizer. 

### c.

```{r}
cornfit <- lm(corndata$Yield ~ corndata$F1 + corndata$F2 + corndata$F3, data=corndata)
print(cornfit)
# plot(corndata$Fertilizer,corndata$Yield,pch=16)
# abline(cornfit,lty="dashed",col="gray")
```

### d.

```{r}
summary(cornfit)
# Extract p-values
# pval_ls <- function (modelobject) {
#     if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
#     f <- summary(modelobject)$fstatistic
#     p <- pf(f[1],f[2],f[3],lower.tail=F)
#     attributes(p) <- NULL
#     return(p)
# }
# ADJUST
```

The p-values determined for coeff 1, 2, and 3 are, respectively: 0.0034, 0.9635, and 0.0242. The p-value for F2 indicator variable does not relay statistical significance, but the values for F1 and F3 do. Therefore, we might reject the null for only F1 and F3 but not F2. 

We use the t-test. 

```{r}
confint(cornfit, level = 0.95)
```

Above is a matrix of confidence intervals for each indicator variable. Overall, there is moderate reason to be confident in the model, though the intervals are large for each coefficient. 

### e. 

```{r}
linearHypothesis(cornfit, c("corndata$F1=corndata$F2","corndata$F2=corndata$F3"))

confint(cornfit, level = 0.95)
```

The funcion above tests H0: F1=F2=F3; F-test used.

### f.

```{r}
fit_common <- lm(corndata$Yield ~ corndata$Fertilizer,data=corndata)
print(summary(fit_common))
plot(corndata$Fertilizer,corndata$Yield,pch=16,col="blue")
abline(fit_common,lty="dashed")
```

The estimate for uF is (-)1.5400, and the p-value associated with this estimate is 0.049, therefore the estimate is statistically significant.

## Problem 2

### a.

```{r}
# Model 1 used
ssr <- 4604.7
sse <- 1604.44
sst <- ssr + sse
r2 <- ssr/sst
slope <- 0.169
r <- (1)*sqrt(r2)
print(paste("R =",round(r,3)))
```

### b.

```{r}
# Model 2 used
predict_m2 <- -4.117+0.174*(100)-3.162*(1)-3.818*(0)+0.311*(0)
print(paste("Estimated price, P =",round(predict_m2,3),"thousands of dollars"))
```

### c.

Holding horsepower fixed, we can look at the predicted coefficients for each country, as displayed in Model 2 to determine which will have the least expensive car. As coefficient decreases, price decreases. 

The coefficients for USA, Japan, and Germany are, respectively, (-)3.162,-3.818,0.311. Thus, Japan will have the cheapest selling cars, while Germany will have the most expensive selling cars. 

### d.

H0: There is no interaction between Country and horespower (HP*country vars are not significant).
HA: At least one of the HP*country vars are significant, and there IS interaction

```{r}
# F-test, partial; predict a group of predictors, all at once
fm_SSE <- 1319.85 # from Model 3
fm_MSE <- 16.0957 # from Model 3
rm_SSE <- 1390.31 # from Model 2
n_preds <- 3
F_stat <- ((rm_SSE-fm_SSE)/n_preds)/fm_MSE
print(paste("F-stat",round(F_stat,3)))
```

F-critical: 3/82 
p-value: 0.232 via F distribution
p>0.05 (alpha), therefore H0 cannot be rejected. Given hypotheses, we can say that there is not sufficient evidence of interaction between country and horsepower, and Model 2 is more significant than Model 3. 

### e.

H0: Country is not an important (significant) predictor of car prices
HA: Country is an important (significant) predictor of car prices

```{r}
# F-test, partial; predict a group of predictors, all at once
fm_SSE <- 1390.31 # from Model 2
fm_MSE <- 16.3566 # from Model 2
rm_SSE <- 1604.44 # from Model 1
n_preds <- 3
F_stat <- ((rm_SSE-fm_SSE)/n_preds)/fm_MSE
print(paste("F-stat",round(F_stat,3)))
```

F-critical: 3/85
p-value: 0.0066 via F distribution
p<0.05 (alpha), therefore H0 can be rejected. HA is accepted, and we can say that Country is a signficiant predictor of gas prices. 

### f.

From Model 2, we know that:

p-val, USA = 0.0216 < 0.05 (alpha).
This confers significance on the regression coefficient, which finds that US cars are cheaper than those made in Other countries. No desire to combine USA and Other. No desire to combine with Others.

p-val, Japan = 0.0061 < 0.05
This confers significance. Japanese cars are cheaper, significantly, therefore no need for combination with Other.

p-val, Germany = 0.8682>0.05, therefore no significance. Thus, Germany could combine from Others 

Yes, recommendd; Germany can be joined with Others to form one category.

## Problem 3

### a.

```{r}
winedata <- fread("http://statistics.uchicago.edu/~burbank/data/s224/wine.txt")
winedata <- data.frame(winedata)
boxcox(lm(winedata$price ~ winedata$vintage_yr,data=winedata))
```

### b.

Follow the ladder of transformation.

First, lambda = (-)1: transformation is 1/Y

```{r}
winedata2 <- mutate(winedata,reciprocalPrice=1/winedata$price,sqrtPrice=sqrt(winedata$price),logTime=log(winedata$price))

lmfit <- lm(reciprocalPrice ~ winedata$vintage_yr,data=winedata2)
print(lmfit)
summary(lmfit)
```

### c.

```{r}
logfit <- lm(log(winedata$price) ~ winedata$vintage_yr,data=winedata)
print(logfit)
summary(logfit)
```

Part b: p-val = 3.15e-05; R2 = 0.7763
Part c: p-val = 2.98e-08; R2 = 0.9231 

The p values for both models indicate statistical significance for the predicted value of the regression coefficient for vintage year. The R^2 values each convey a high level of relatedness between variables, but this value increases when log(price) is used as the response variable in part c.

## Problem 4

### a.

```{r}
ad_data <- fread("https://tinyurl.com/magazinedata")
ad_fit <- lm(ad_data$R ~ ad_data$P, data=ad_data)
plot(ad_data$P,ad_data$R, pch=16, col="blue")
abline(ad_fit,lty="dashed")
summary(ad_fit)
```

This is likely an inaccurate fit, as data is skewed to the lower portion of the domain of price data. One outlier point causes the graph shown above to cover a much larger area of price than would otherwise be covered. There does still seem to be, however, an approximately linear relationship between the variables--just not the one that is currently being regressed. 

```{r}
plot(fitted(ad_fit),resid(ad_fit),col="blue",pch=16)
abline(h=0,lty="dashed")
```

Per the graph above, there does seem to be heteroscedasticity. The outlier residual is notable, however, but heteroscedasticity will likely persist to some extent, even when removed.

### b.

There is heteroscedasticity, per the above observation. Therefore, the constant variance assumption/homoscedasticity is violated. Therefore, we use a transformation to achieve constant variance (homoscedasticity). We want a transformation that preserves the linear relationship between response and predictor, and results in residuals exhibiting constant variance.

For the purposes of this question, we will use logarithm to transform the data. 

```{r}
lmfit <- lm(log(ad_data$R) ~ log(ad_data$P), data=ad_data)
print(lmfit)
summary(lmfit)
plot(log(ad_data$P),log(ad_data$R), pch=16, col="red")
abline(lmfit,lty="dashed")
# plot(lmfit)
plot(fitted(lmfit),resid(lmfit),col="red",pch=16)
abline(h=0,lty="dashed")
```

The new R^2 value for the transformed data is 0.4203, which indicates elevated performance relative to the value for the original data, 0.1263 Still, the new R^2 is not an indicator of an objectively strong relationship between variables. Our graph of fitted vs. residuals seems to demonstrate that not all heteroscedasticity may have been removed from the model. 

### c.

```{r}
# Determine outliers
plot(lmfit,pch=16)
# Remove outliers; datapoints
ad_data3 <- ad_data[-c(15,23,41),]
lmfit2 <- lm(log(ad_data3$R) ~ log(ad_data3$P), data=ad_data3)
# demonstrate assumptions still met
plot(lmfit2,col="blue",pch=16)
```

Plot of the fit demonstrates that the assumptions of regression are still met, and are actually more closely adhered to when the outliers are removed from the data, but only for some assumptions. Others are violated more blatantly The transformed data is used for both sets of analyses. 

### d.

```{r}
ad_data[c(15,23,41),]
```

Each outlier is characterzed by their datapoint in one variable or another being uncharacteristically extreme in a way that does not match the rest of the datapoints (within the general vicinity, at least). Outliers exist in some domain or range of the data in which one of the variables making up a datapoint is far more extreme than for the datapoints immediately around it. This is the case for the three points chosen, and their removal played a large impact on the fit as a result. 

### e.

```{r}
# Pre
summary(lmfit)
# Post-removal
summary(lmfit2)
```

Pre-Removal: 0.4203 

Post-Removal: 0.6113

Thus, there is an increase in the R^2 value that is appreciable (the latter value is roughly 145% greater than the former). 

I would continue using P as a predictor of R while removing outliers, as it seems to do a good job of fitting in the majority of the data. The Magazine variable is not necessary except for identification.

### f.

```{r}
lm(ad_data$R ~ ad_data$P,data=ad_data)
```

R = 7.6041 + 0.3527(unit advertising) + error

Given the equation above, we know there is a 0.3527 increase in R for each additional page of ads (P). 

## Problem 5

```{r}
thedata <- read.delim("http://statistics.uchicago.edu/~burbank/data/RABE5/P152.txt")
thedata <- thedata %>% filter(STATE != "AK") %>% droplevels() # drop AK observation
```

### a.

```{r}
lmfit <- lm(Y ~ X1 + X2 + X3, data = thedata)
tidy(lmfit)
# glance(lmfit)
glance(lmfit)[c(1,2,3,4,5,6,11)]
```

### b.

```{r}
thedata <- mutate(thedata, r = rstandard(lmfit), yhat = fitted(lmfit))

p1 <- gf_point(r ~ yhat, data=thedata)
p2 <- gf_point(r ~ Region, data=thedata)
p3 <- gf_point(r ~ X1, data=thedata)
p4 <- gf_point(r ~ X2, data=thedata)
p5 <- gf_point(r ~ X3, data=thedata)
grid.arrange(p1, p2, p3, p4, p5, ncol=2)
```

### c.

The plot associate with X1 appears to display a rough fan shape. Therefore, it is likely that variance does increase with yhat. 

### d.

Re-ordering 'thedata' in RStudio reveals, overall and somewhat subjectively, that the largest values for X1 are generally associated with Region 1, the largest values for X2 are generally associated with Regions 3/4, and a more diverse set of regions correspond with the largest values for X3. There is roughly similar variance between the various predictors, and it is expected that there will be similar variance between the regions, similarly. (Thus, I've drawn from the plot of region x residuals in conjunction with the plots of predictors x residuals).

### e.

```{r}
thedata <- mutate(thedata, e = residuals(lmfit))
thedata <- thedata %>%
   mutate( s2 = mean(e^2) ) %>%
   add_count(Region)   %>%
   group_by(Region) %>%
   mutate(s2j = sum(e^2) / (n-1), cj = sqrt(s2j / s2) ) %>%
   ungroup()
thedata <- thedata %>% mutate(wt = 1 / cj^2)
lmfit.wt <- lm(Y ~ X1 + X2 + X3, data = thedata, weights = wt)
tidy(lmfit.wt)
thedata
```

c1 = 1.2393134
c2 = 1.1552806
c3 = 0.6308328
c4 = 1.2083860

Both models convey significance, but there is greater significance in the latter model.

### f.

```{r}
thedata <- mutate(thedata, e.wt = residuals(lmfit.wt))
mean.Y <- mean(~ Y, data = thedata)
SST <- sum(~ (Y - mean.Y)^2, data = thedata)
mean.e.wt <- mean(~ e.wt, data = thedata)
print(paste("Weighted Mean=",mean.e.wt))
SSE <- sum(~ (e.wt - mean.e.wt)^2, data = thedata)
# R2 = 1 − SSE/SST
rsquared <- 1-(SSE/SST)
print(paste("R^2 =",round(rsquared,3)))
# sighat = sqrt(SSE/(n − p − 1))
n <- 49
p <- 3
sighat <- sqrt(SSE/(n-p-1))
print(paste("Sighat =", round(sighat,3)))
```

Compare with A: 
R^2 = 0.5523623
sigma = 27.04515

Thus, the results between the two models are highly similar and, interestingly, the former model conveys a marginally stronger relationship between variables.

### g.

```{r}
thedata <- mutate(thedata, e.wtd = residuals(lmfit.wt), yhat.wtd = fitted(lmfit.wt))
thedata <- mutate(thedata, e.wtd.adj = e.wtd * sqrt(wt))
p1 <- gf_point(e.wtd.adj ~ yhat.wtd, data=thedata)
p2 <- gf_point(e.wtd.adj ~ Region, data=thedata)
p3 <- gf_point(e.wtd.adj ~ X1, data=thedata)
p4 <- gf_point(e.wtd.adj ~ X2, data=thedata)
p5 <- gf_point(e.wtd.adj ~ X3, data=thedata)
grid.arrange(p1, p2, p3, p4, p5, ncol=2)
```

There does not appear to be a severe fan shape in the data for any of the predictors, and the regions appear to be even more equally distributed. Thus, the problems that existed previously have been corrected for. 

```{R}
#update.packages(ask = FALSE, checkBuilt = TRUE, force = TRUE)
# tinytex::tlmgr_update()
```



